# 2.2 Архитектурные решения (core)

Документ описывает архитектурные решения сервиса `core` в составе платформы записи (`appointment-platform/services/core`) на основании текущей кодовой базы.

## Тип архитектуры

- **На уровне платформы**: микросервисная архитектура (каталог `services/*`).
- **На уровне `core`**: клиент‑серверный сервис с **gRPC API** и отдельной БД (PostgreSQL).
- **Состояние сервиса**: `core` проектируется как **стейтлесс‑процесс**, состояние хранится в БД; это позволяет горизонтальное масштабирование при общей БД.

## Используемые технологии и подходы

- **Go** (`go.mod`) как основной язык.
- **gRPC + Protobuf**: контракты в `internal/api/**` (gRPC reflection включён; инициализация в `cmd/main.go`).
- **GORM** как ORM; драйвер БД — `gorm.io/driver/postgres` (в тестах местами используется SQLite).
- **UUID** как основные идентификаторы доменных сущностей (`github.com/google/uuid`).
- **Хранение правил расписания**:
  - в API правило представлено как `common.v1.ScheduleRule` (`internal/api/common/v1/common.proto`);
  - в БД правило хранится в JSONB поле `schedules.rules` (`internal/model/schedule.go`);
  - кодек JSON ↔︎ proto реализован в `internal/service/calendar_service.go` (`encodeScheduleRule`/`decodeScheduleRule`).
- **Время**:
  - `gorm` настроен на `UTC` через `NowFunc` (`internal/db/db.go`);
  - в БД слоты/бронирования хранятся в UTC (`timestamp with time zone`), а локальные TZ используются при развёртывании правил расписания.

## Слои приложения

Текущая структура соответствует слоистой архитектуре “transport → application/service → data access → persistence model”.

1) **Транспортный слой (представление / API)**
   - gRPC‑сервер и регистрация сервисов: `cmd/main.go`.
   - `.proto` и сгенерированный код: `internal/api/**`.
   - Маппинг доменных моделей в protobuf‑DTO и обратно — внутри `internal/service/*` (например, `mapBooking`, `mapSlot`, `mapProviderSchedule`).

2) **Слой бизнес‑логики (application/service)**
   - Реализации RPC‑методов:
     - `internal/service/calendar_service.go` (`CalendarService`)
     - `internal/service/identity_service.go` (`IdentityService`)
   - Валидации входных параметров и перевод ошибок в gRPC‑коды (`codes.InvalidArgument`, `codes.NotFound`, …).
   - Чистая логика календаря вынесена в утилиты (`internal/utils` и частично `internal/calendar`) и покрыта тестами.

3) **Доступ к данным (data access)**
   - Репозитории (интерфейсы + GORM‑реализации) в `internal/repository/*`.
   - Репозитории скрывают детали SQL/GORM от сервисного слоя и упрощают тестирование.

4) **Слой хранения (persistence model)**
   - GORM‑модели: `internal/model/*`.
   - Автомиграции на старте: `internal/model/migrate.go` вызывается из `cmd/main.go`.

Упрощённая схема вызовов:

`gRPC client` → `CalendarService/IdentityService` → `Repository (GORM)` → `PostgreSQL`.

## Ключевые модели и данные

Основные сущности (по `internal/model/*`):

- Пользователи и роли: `users`, `roles`, `user_roles`.
- Акторы календаря: `clients`, `providers`.
- Каталог: `services`, связка `provider_services`.
- Расписания и слоты: `schedules` (правила), `time_slots` (материализованные интервалы).
- Бронирования: `bookings`.
- Аудит/события: `events` (заведена модель; использование зависит от методов сервиса).

## Архитектурные решения внутри домена календаря

### “Правило + материализация слотов”

`core` хранит расписание провайдера как правило повторения (`ScheduleRule`) и **при необходимости материализует** `time_slots` в заданном окне:

- Метод `ListFreeSlots` сначала разворачивает расписания провайдера в окне и создаёт недостающие `time_slots`, затем читает их из репозитория с пагинацией.
- Такой подход ускоряет выдачу свободных слотов и даёт единый источник истины для бронирований (`time_slots`), но требует контроля конкуренции и согласованности данных при параллельных запросах/репликах.

### Конкурентная защита при бронировании

`CreateBooking` использует транзакцию и блокировку строки слота (`SELECT ... FOR UPDATE`) через `clause.Locking{Strength: "UPDATE"}`:

- предотвращает двойное бронирование одного слота при параллельных запросах;
- внутри транзакции выполняются проверки конфликтов по времени и перевод статуса слота в `booked`.

## Подход к обеспечению отказоустойчивости и восстановлению

Ниже — то, что уже реализовано в коде, и то, что обычно требуется для эксплуатации.

### Реализовано сейчас

- **Стейтлесс‑процесс**: сервис не хранит сессии/кэш в памяти как единственный источник данных; все критичные данные — в БД.
- **Graceful shutdown**: корректная остановка gRPC‑сервера по SIGINT/SIGTERM (`cmd/main.go`), что снижает вероятность оборванных запросов.
- **Пулы соединений**: настройки `MaxOpenConns/MaxIdleConns/ConnMaxLifetime` (`internal/db/db.go`).
- **Транзакции** на критических операциях:
  - `CreateBooking` (бронирование + смена статуса слота),
  - `BulkCancelProviderSlots` (отмена слотов + связанных бронирований),
  - материализация слотов в окне (внутри транзакции) при `ListFreeSlots`.
- **Контейнеризация**: `Dockerfile` собирает статический бинарник и запускает его в distroless образе, что упрощает рестарт/раскатку.

### Риски/ограничения текущей реализации (важно учитывать)

- **Зависимость от Postgres**: БД — основной SPOF (single point of failure) для `core`; при недоступности БД сервис фактически неработоспособен.
- **Автомиграции на старте**: `AutoMigrate` в `cmd/main.go` может:
  - увеличивать время старта;
  - конкурировать между несколькими репликами при одновременном запуске.
- **Материализация слотов при горизонтальном масштабировании**:
  - сейчас в `time_slots` нет явного уникального ограничения, которое бы гарантировало отсутствие дублей слотов при параллельной материализации несколькими экземплярами;
  - при высоких нагрузках это может привести к дублям и “грязным” данным (лечится схемой БД).

### Рекомендованный подход к отказоустойчивости (для production)

- **Запуск нескольких реплик `core`** за балансировщиком (L4/L7) — за счёт стейтлесс‑характера сервиса.
- **Readiness/Liveness probes** (health‑endpoint или gRPC health checking) и корректные таймауты на стороне клиентов.
- **Устойчивость БД**:
  - репликация/managed Postgres,
  - регулярные бэкапы + проверка восстановления,
  - мониторинг соединений/латентности.
- **Схемные гарантии целостности**:
  - добавить уникальный индекс для `time_slots` (например, `(provider_id, service_id, starts_at, ends_at)` или другой согласованный ключ),
  - продумать стратегию идемпотентности/дедупликации для материализации.
- **Логи/трассировка/метрики**:
  - структурированные логи (вместо `log.Printf`),
  - метрики по ошибкам RPC, времени ответов, пулу соединений,
  - корреляция запросов (request_id/trace_id).

## Вывод

`core` — отдельный микросервис календарного домена с gRPC‑API, реализованный как слоистое приложение на Go: транспорт (protobuf/gRPC) → сервисы (бизнес‑логика) → репозитории (GORM) → Postgres (персистентность). Для отказоустойчивости база — стейтлесс‑процесс, транзакции и корректная остановка; для production‑эксплуатации критично усилить операционные практики (HA Postgres, health checks, наблюдаемость) и целостность схемы (уникальные ограничения на слоты).
