# 4.4 Безопасность и надежность (сервис `core`)

Документ отвечает на вопросы раздела 4.4: защита данных, аутентификация и авторизация, резервирование и аварийное восстановление, а также политики безопасности (шифрование, логины/пароли/2FA, регулярные бэкапы и восстановление). Описание основано на фактической реализации `services/core` и практиках эксплуатации.

## 4.4.1 Защита данных

### Передача данных (in transit)

**Фактически реализовано в `core`:**
- gRPC‑сервер поднимается на TCP‑адресе (по умолчанию `:50051`) без TLS на уровне приложения (`cmd/main.go` использует `grpc.NewServer()` без credentials).
- Включён gRPC reflection (`reflection.Register`), что упрощает диагностику через `grpcurl`, но увеличивает поверхность атаки при публичной доступности.

**Политика безопасности (рекомендуется для production):**
- Шифрование транспорта:
  - включить TLS для gRPC (желательно mTLS для межсервисного взаимодействия), либо
  - использовать TLS‑терминацию на API‑gateway/ingress (при этом защищать внутреннюю сеть).
- Ограничить reflection:
  - отключить в production окружении, либо
  - оставить включённым, но закрыть сетевыми политиками (только из доверенной сети) и/или аутентификацией на шлюзе.
- Сегментация сети:
  - запрет прямого доступа к gRPC порту из интернета;
  - разрешить доступ только из доверенных компонентов (бот, gateway, сервис‑mesh).

### Хранение данных (at rest)

**Фактически реализовано в `core`:**
- Основное хранилище — PostgreSQL; параметры подключения берутся из переменных окружения (`internal/config/db.go`).
- UUID генерируются на стороне БД (`gen_random_uuid()` в GORM‑моделях), поэтому для Postgres обычно требуется расширение `pgcrypto`.
- Приложение не реализует шифрование полей (например, телефона) и не хранит секреты в отдельном хранилище; ожидается, что секреты предоставляет окружение.

**Политика безопасности (рекомендуется для production):**
- Защитить канал к БД:
  - включить SSL/TLS на Postgres (`DB_SSLMODE=require`/`verify-full`);
  - управлять сертификатами (CA, client certs) в инфраструктуре.
- Защитить данные “at rest”:
  - шифрование дисков/томов БД (или managed‑encryption у облачного провайдера);
  - шифрование бэкапов.
- Защитить PII:
  - минимизировать доступ к `users.contact_phone`;
  - маскировать телефон в логах/трассировке;
  - при повышенных требованиях — токенизация/шифрование поля телефона на прикладном уровне.
- Управление секретами:
  - хранить `DB_PASSWORD` и другие секреты в Secret‑хранилище (Kubernetes Secrets/Vault/SSM);
  - настроить ротацию паролей и ограничение прав БД‑пользователя (минимальные привилегии).

## 4.4.2 Аутентификация и авторизация

### Текущая модель (фактическая реализация)

**Идентификация пользователя:**
- Пользователь идентифицируется по `telegram_id` (таблица `users`), методы `IdentityService` работают по Telegram ID.

**Роли:**
- Роли хранятся в `roles` и `user_roles`; политика “одна роль на пользователя” реализована в репозитории (удаление старых связей и создание новой).

**Авторизация в `CalendarService`:**
- Для методов управления расписаниями/слотами выполняется проверка роли владельца `provider_id`:
  - `CalendarService.ensureProviderRole` загружает `provider.user_id` и проверяет, что роль пользователя — `"provider"`.

**Ограничение реализации (важно):**
- В `core` отсутствует аутентификация вызывающей стороны (нет JWT/mTLS/подписей запросов).
- Нет привязки “кто вызывает RPC” к конкретному `telegram_id`/`user_id` и нет централизованного RBAC/ABAC на уровне всех методов.
- Поэтому защита “кто может вызвать метод и с какими ID” должна обеспечиваться внешним слоем (бот/шлюз) или дополняться в `core`.

### Политика аутентификации (рекомендуется)

Варианты:
- **Через gateway/edge‑сервис**: gateway аутентифицирует пользователя (Telegram/OIDC), выпускает/проверяет JWT и проксирует запросы в `core`.
- **На уровне `core`**: внедрить gRPC interceptor, который валидирует токен/сертификат и формирует `principal` (например: `user_id`, `telegram_id`, `role`).

Рекомендуемые данные идентичности (claims/metadata), которые нужны для авторизации:
- `subject` (`user_id`) и/или `telegram_id`;
- `role`;
- при необходимости — `client_id` и `provider_id`, полученные из БД и закреплённые за субъектом.

### Политика авторизации (рекомендуется)

Рекомендуется сочетание RBAC + проверка владения ресурсом (ABAC):
- **Клиент**:
  - может создавать бронирование только для своего `client_id`;
  - может просматривать/отменять только свои `booking_id` (где `bookings.client_id == client_id`).
- **Провайдер**:
  - может создавать/обновлять/удалять только свои слоты (`time_slots.provider_id`);
  - может управлять только своими расписаниями (`schedules.provider_id`);
  - массовая отмена — только по своему `provider_id`.
- **Администратор** (если требуется проектом):
  - расширенные операции (например, назначение роли, просмотр по всем пользователям и т.п.).

Технические меры:
- единая точка контроля (interceptor) + контекст пользователя в handler’ах;
- аудит критических действий (назначение роли, отмены, изменения расписаний/слотов).

## 4.4.3 Надежность (устойчивость к сбоям) и целостность данных

### Что реализовано

**Стейтлесс‑процесс:**
- `core` не хранит сессии как источник истины; состояние — в БД. Это упрощает горизонтальное масштабирование.

**Транзакционность:**
- Критические операции выполняются транзакционно:
  - создание бронирования: транзакция + блокировка строки слота (`FOR UPDATE`), создание `booking`, перевод слота в `booked`;
  - отмена бронирования: транзакция (отмена `booking` + перевод слота в `planned`);
  - массовая отмена: транзакция (сбор затронутых записей + отмена бронирований + отмена слотов).

**Схемные гарантии:**
- Уникальность `bookings.slot_id` предотвращает более одного бронирования на слот на уровне БД.

**Управление соединениями:**
- Используется пул соединений Postgres (max open/idle/lifetime настраиваются env).

**Управление жизненным циклом:**
- Graceful shutdown gRPC сервера при SIGINT/SIGTERM.

### Политики повышения надежности (рекомендуется)

- Health checks:
  - добавить gRPC Health Checking сервис или отдельный endpoint на gateway;
  - readiness должна зависеть от доступности БД.
- Наблюдаемость:
  - метрики по RPC (latency/error rate), метрики БД, структурированные логи;
  - распределённая трассировка (OpenTelemetry) для диагностики end‑to‑end.
- Управление миграциями:
  - `AutoMigrate` на старте удобен для разработки, но для production лучше управляемые миграции (отдельный job/step).
- Ограничение нагрузки:
  - rate limiting на gateway;
  - ограничения размеров запросов/окна времени, чтобы избежать тяжёлых materialize‑операций при очень больших интервалах.

## 4.4.4 Резервирование и аварийное восстановление (backup & DR)

### Регулярные бэкапы (политика)

Так как `core` хранит критическое состояние в PostgreSQL, резервирование относится в первую очередь к БД.

Рекомендуемая политика (примерная, уточняется требованиями проекта):
- **Base backup / snapshot**: регулярные полные снимки БД (например, ежедневно).
- **PITR (WAL‑архивация)**: хранение WAL для point‑in‑time recovery при требованиях минимального RPO.
- **Ротация и хранение**:
  - retention (например, 7/14/30 дней) по требованиям;
  - хранение в отдельной зоне/регионе (защита от инцидентов площадки).
- **Безопасность бэкапов**:
  - шифрование бэкапов “at rest”;
  - ограничение доступа (только сервисные аккаунты, аудит доступа).

### Восстановление (политика)

**Тестовые восстановления:**
- Регулярно выполнять восстановление на тестовый стенд и проверять:
  - что БД поднимается;
  - что схема/данные консистентны;
  - что базовые RPC сценарии работают (регистрация, выдача слотов, создание/отмена брони).

**Runbook аварийного восстановления:**
- Шаги восстановления:
  1) восстановить базовый бэкап;
  2) (если PITR) применить WAL до нужной точки;
  3) проверить целостность и доступность;
  4) переключить `core` на восстановленный Postgres (или переключить DNS/service endpoints).
- Определить и зафиксировать:
  - **RPO** (допустимая потеря данных по времени),
  - **RTO** (время восстановления сервиса).

**Резервирование Postgres (HA):**
- Для production рекомендуется HA‑Postgres (репликация/managed‑service) и мониторинг:
  - latency, connections, disk usage, replication lag.
